{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "f=open('F:/contest/langchao/test.csv','r',encoding='gb18030', errors = 'ignore')\n",
    "train = pd.read_csv(\"F:/contest/langchao/train.csv\", encoding = 'gb18030')\n",
    "test = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\python\\venv\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "from  keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import  pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GlobalMaxPool1D,GRU, Embedding,Bidirectional, Flatten,LSTM, BatchNormalization,Conv1D,MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import *\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg\n",
    "import jieba.analyse\n",
    "import codecs\n",
    "from keras.layers import Input, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\LIUJIA~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.365 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "max_features = 80000 ## 词汇量  \n",
    "maxlen = 150  ## 最大长度\n",
    "embed_size = 200 # emb 长度\n",
    "\n",
    "def splitWord(query, stopwords):\n",
    "    wordList = jieba.cut(query)\n",
    "    num = 0\n",
    "    result = ''\n",
    "    for word in wordList:\n",
    "        word = word.rstrip()\n",
    "        word = word.rstrip('\"')\n",
    "        if word not in stopwords:\n",
    "            if num == 0:\n",
    "                result = word\n",
    "                num = 1\n",
    "            else:\n",
    "                result = result + ' ' + word\n",
    "    return result\n",
    "def preprocess(data):\n",
    "    stopwords = {}\n",
    "    for line in codecs.open('F:/contest/langchao/stop.txt','r','gb18030'):\n",
    "        stopwords[line.rstrip()]=1    \n",
    "    data['doc'] = data['Discuss'].map(lambda x:splitWord(x,stopwords))\n",
    "    return data\n",
    "\n",
    "train.Discuss.fillna('_na_',inplace=True)\n",
    "test.Discuss.fillna('_na_',inplace=True)\n",
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "\n",
    "comment_text = np.hstack([train.doc.values])\n",
    "tok_raw = Tokenizer(num_words=max_features)\n",
    "tok_raw.fit_on_texts(comment_text)\n",
    "train['Discuss_seq'] = tok_raw.texts_to_sequences(train.doc.values)\n",
    "test['Discuss_seq'] = tok_raw.texts_to_sequences(test.doc.values)\n",
    "\n",
    "\n",
    "def get_keras_data(dataset): \n",
    "    X={\n",
    "        'Discuss_seq':pad_sequences(dataset.Discuss_seq,maxlen=maxlen)\n",
    "    }\n",
    "    return X\n",
    "\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    return 1.0/(1.0+K.sqrt(K.mean(K.square(y_true - y_pred), axis=-1)))\n",
    "\n",
    "def cnn():\n",
    "    #Inputs\n",
    "    comment_seq = Input(shape=[maxlen],name='Discuss_seq')\n",
    "    \n",
    "    #Embeddings layers\n",
    "    emb_comment =Embedding(max_features, embed_size)(comment_seq)\n",
    "    \n",
    "    # conv layers\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5]\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=100,kernel_size=fsz,activation='relu')(emb_comment)\n",
    "        l_pool = MaxPooling1D(maxlen-fsz+1)(l_conv)\n",
    "        l_pool = Flatten()(l_pool)\n",
    "        convs.append(l_pool)\n",
    "    merge =concatenate(convs,axis=1)\n",
    "    \n",
    "    out = Dropout(0.5)(merge)\n",
    "    output  = Dense(32,activation='relu')(out)\n",
    "    \n",
    "    output = Dense(units=1,activation='linear')(output)\n",
    "    \n",
    "    model = Model([comment_seq],output)\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", score])\n",
    "    return model\n",
    "\n",
    "\n",
    "def rnn1():\n",
    "    comment_seq = Input(shape=[maxlen],name='Discuss_seq')\n",
    "    \n",
    "    #Embeddings layers\n",
    "    emb_comment =Embedding(max_features, embed_size, weights=[embedding_matrix])(comment_seq)\n",
    "    main = Bidirectional(GRU(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.4))(emb_comment)\n",
    "#     main1 = Bidirectional(LSTM(32, return_sequences=True, dropout=0.1, recurrent_dropout=0.2))(emb_comment)\n",
    "    main = GlobalMaxPool1D()(main)\n",
    "#     main1 = GlobalMaxPool1D()(main1)\n",
    "#     main =concatenate([main,main1],axis=1)\n",
    "    main = Dense(50, activation=\"relu\")(main)\n",
    "    main= Dropout(0.2)(main)\n",
    "    main = Dense(units=1,activation='linear')(main)\n",
    "    model = Model([comment_seq],main)\n",
    "    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", score])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "X_train =get_keras_data(train)\n",
    "X_test = get_keras_data(test)\n",
    "y_train = train.Score.values\n",
    "\n",
    "\n",
    "# batch_size = 128 \n",
    "# epochs = 20\n",
    "# early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "\n",
    "# callbacks_list = [early_stopping]\n",
    "# model = cnn()\n",
    "# model.summary()\n",
    "# model.fit(X_train, y_train,\n",
    "#             validation_split=0.1,\n",
    "#             batch_size=batch_size, \n",
    "#             epochs=epochs, \n",
    "#             eval_set = \n",
    "#             shuffle = True,\n",
    "#             callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Discuss_seq (InputLayer)        (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 200)     16000000    Discuss_seq[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 149, 100)     40100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 148, 100)     60100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 147, 100)     80100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 146, 100)     100100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 100)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 100)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 100)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 100)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 100)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 100)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 100)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 100)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 400)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 400)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           12832       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            33          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,293,265\n",
      "Trainable params: 16,293,265\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18000/18000 [==============================] - 202s 11ms/step - loss: 0.6687 - mean_absolute_error: 0.6539 - score: 0.6526 - val_loss: 0.5813 - val_mean_absolute_error: 0.6442 - val_score: 0.6458\n",
      "Epoch 2/20\n",
      "18000/18000 [==============================] - 214s 12ms/step - loss: 0.3032 - mean_absolute_error: 0.4339 - score: 0.7320 - val_loss: 0.5728 - val_mean_absolute_error: 0.6197 - val_score: 0.6600\n",
      "Epoch 3/20\n",
      "18000/18000 [==============================] - 206s 11ms/step - loss: 0.2015 - mean_absolute_error: 0.3416 - score: 0.7752 - val_loss: 0.6471 - val_mean_absolute_error: 0.6572 - val_score: 0.6491\n",
      "Epoch 4/20\n",
      "18000/18000 [==============================] - 214s 12ms/step - loss: 0.1524 - mean_absolute_error: 0.2935 - score: 0.7988 - val_loss: 0.7024 - val_mean_absolute_error: 0.6827 - val_score: 0.6426\n",
      "Wall time: 13min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 128 \n",
    "epochs = 20\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "\n",
    "callbacks_list = [early_stopping]\n",
    "model = cnn()\n",
    "model.summary()\n",
    "model.fit(X_train, y_train,\n",
    "            validation_split=0.1,\n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs, \n",
    "            shuffle = True,\n",
    "            verbose = 1,\n",
    "            callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7f1477894706>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# submission =pd.DataFrame(test.Id.values,columns=['Id'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# submission['Score'] = preds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'F:/contest/langchao/cnn-baseline.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "# preds = model.predict(X_test)\n",
    "# submission =pd.DataFrame(test.Id.values,columns=['Id'])\n",
    "# submission['Score'] = preds\n",
    "res.to_csv('F:/contest/langchao/cnn-baseline.txt',index=None,header =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in preds:\n",
    "    res.append(int(round(i[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
